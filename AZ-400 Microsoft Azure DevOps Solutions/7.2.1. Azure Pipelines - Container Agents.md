# Container agents

- The agent will first fetch and start the container.
  - Then, each step of the job will run inside the container
    - or you can set agent on task level

## Linux agents

- E.g.:

  ```yaml
    pool:
      vmImage: 'ubuntu-16.04'
    container: ubuntu:16.04
    steps:
    - script: printenv
  ```

- On your agent host, ensure:
  - Docker is installed
  - Agent has permission to access the Docker daemon
- Container requirements:
  - Bash
  - `glibc`-based
  - Can run Node.js (which the agent provides)
  - Does not define an `ENTRYPOINT`
  - `USER` has access to `groupadd` and other privileges commands without `sudo`

## Windows agents

- E.g.

  ```yaml
    pool:
      vmImage: 'windows-2019'
    container: mcr.microsoft.com/windows/servercore:ltsc2019
    steps:
    - script: set
  ```

## Service containers

- Spin up multiple containers
- Automatically create, network, and manage the lifecycle
- Read more: [Microsoft documentation](https://docs.microsoft.com/en-us/azure/devops/pipelines/process/service-containers)

### Service container example

```yaml
  resources:
    containers:
    - container: my_container
      image: ubuntu:16.04
    - container: nginx
      image: nginx
    - container: redis
      image: redis

  pool:
    vmImage: 'ubuntu-16.04'

  container: my_container

  services:
    nginx: nginx
    redis: redis

  steps:
  - script: |
      apt install -y curl
      curl nginx
      apt install redis-tools
      redis-cli -h redis ping
```

- Fetches the latest `nginx` and `redis` containers from Docker Hub and then starts the containers
- The containers are networked together so that they can reach each other by their `services` name.
- Pipeline then runs the `apt`, `curl` and `redis-cli` commands inside the `ubuntu:16.04` container.
- From inside this job container, the `nginx` and `redis` host names resolve to the correct services using Docker networking
- All containers on the network automatically expose all ports to each other
